# The Analytics Edge: Assignment 2 {.unnumbered}

The following link will lead you to the assignment on the edX website: [https://learning.edx.org/course/course-v1:MITx+15.071x+2T2020/block-v1:MITx+15.071x+2T2020+type\@sequential+block\@60d93a44280348d7a0a16663f92af0f7](https://learning.edx.org/course/course-v1:MITx+15.071x+2T2020/block-v1:MITx+15.071x+2T2020+type@sequential+block@60d93a44280348d7a0a16663f92af0f7)

## Climate Change

There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.

In this problem, we will attempt to study the relationship between average global temperature and several other factors.

The file [climate_change.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@climate_change.csv) contains climate data from May 1983 to December 2008.

***Start:***\
We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv into R.

```{r}
climateChange <- read.csv("/cloud/project/analyticsedge/Datasets/DatasetsUnit2/climate_change.csv")
```

Then, split the data into a *training set*, consisting of all the observations up to and including 2006, and a *testing set* consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.

```{r}
climateTrain <- subset(climateChange, Year <=2006)
climateTest <- subset(climateChange, Year > 2006)
```

Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.

```{r}
climateModel1 <- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climateTrain)
```

**1.1: What is the model R2 (the "Multiple R-squared" value)?**\
Answer: 0.7509

**1.2: Which variables are significant in the model?**\
Answer: MEI, CO2, CFC.11, CFC.12, TSI, Aerosols

```{r}
summary(climateModel1)
```

Current scientific opinion is that nitrous oxide and CFC-11 are greenhouse gases: gases that are able to trap heat from the sun and contribute to the heating of the Earth. However, the regression coefficients of both the N2O and CFC-11 variables are *negative*, indicating that increasing atmospheric concentrations of either of these two compounds is associated with lower global temperatures.

**2.1: Which of the following is the simplest correct explanation for this contradiction?**\
Answer: All of the gas concentration variables reflect human development - N2O and CFC.11 are correlated with other variables in the data set.

**2.2.1: Which of the following independent variables is N2O highly correlated with (absolute correlation greater than 0.7)?**\
Answer: CO2, CH4, CFC.12

**2.2.2: Which of the following independent variables is CFC.11 highly correlated with?**\
Answer: CH4, CFC.12

```{r}
cor(climateTrain)
```

Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Remember to use the training set to build the model.

```{r}
climateModel2 <- lm(Temp ~ MEI + N2O + TSI + Aerosols, data = climateTrain)
```

**3.1: What is the coefficient of N2O in this reduced model?**\
Answer: 2.532e-02 (0.02532)

**3.2: What is the model R\^2?**\
Answer: 0.7261

```{r}
summary(climateModel2)
```

We have many variables in this problem, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2. This trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model. 

Use the step function in R to derive a new model, with the full model as the initial model.

```{r}
climateModel <- step(climateModel1)
```

**4.1: What is the R\^2 value of the model produced by the step function?**\
Answer: 0.7508

**4.2: Which of the variable(s) were eliminated from the full model by the step function?**\
Answer: CH4

```{r}
summary(climateModel)
```

**5: Using the model produced from the step function, calculate temperature predictions for the testing data set, using the predict function. What is the testing set R\^2?**\
Answer: 0.6286051

```{r}
predictTemp <- predict(climateModel, newdata = climateTest)
SSE = sum((predictTemp - climateTest$Temp)^2)
SST = sum((mean(climateTrain$Temp) - climateTest$Temp)^2)
R2 = 1 - SSE/SST
R2
```

## Reading Test Scores

The Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in mathematics, reading, and science. This test provides a quantitative way to compare the performance of students from different parts of the world. In this homework assignment, we will predict the reading scores of students from the United States of America on the 2009 PISA exam.

The datasets [pisa2009train.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009train.csv) and [pisa2009test.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009test.csv) contain information about the demographics and schools for American students taking the exam, derived from [2009 PISA Public-Use Data Files](http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2011038) distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, by using the data you are bound by them [NCES data use agreement](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@NCES_Data_Use_Agreement.txt), which prohibits any attempt to determine the identity of any student in the datasets.

***Start:***\
Load the training and testing sets [pisa2009train.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009train.csv) and [pisa2009test.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009test.csv) using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.

```{r}
pisaTrain <- read.csv("/cloud/project/analyticsedge/Datasets/DatasetsUnit2/pisa2009train.csv")
pisaTest <- read.csv("/cloud/project/analyticsedge/Datasets/DatasetsUnit2/pisa2009test.csv")
```

**1.1: How many students are there in the training set?**\
Answer: 3663
```{r}
nrow(pisaTrain)
```

**1.2.1: Using tapply() on pisaTrain, what is the average reading test score of males**\
Answer: 483.5325

**1.2.2: Using tapply() on pisaTrain, what is the average reading test score of females?**\
Answer: 512.9406
```{r}
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
```

**1.3: Which variables are missing data in at least one observation in the training set?**\
Answer: *Check output*
```{r}
names(which(colSums(is.na(pisaTrain)) > 0))
```

Linear regression discards observations with missing data, so we will remove all such observations from the training and testing sets.
```{r}
pisaTrain <- na.omit(pisaTrain)
pisaTest <- na.omit(pisaTest)
```


**1.4: How many observations are now in the training/testing set?**\
Answer: 2414/990 respectively
```{r}
nrow(pisaTrain)
nrow(pisaTest)
```

**2.1: Which of the variables *grade*, *male* and *raceeth* is an unordered or ordered factor with a min. of 3 values**\
Answer: 
grade - ordered (ex. 8, 9, 10, 11)
male - only has 2 values
raceeth - unordered (no way to sepcifically order it)

\
To include unordered factors in a linear regression model, we define one level as the "reference level" and add a binary variable for each of the remaining levels. In this way, a factor with n levels is replaced by n-1 binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.

As an example, consider the unordered factor variable "color", with levels "red", "green", and "blue". If "green" were the reference level, then we would add binary variables "colorred" and "colorblue" to a linear regression problem. All red examples would have colorred=1 and colorblue=0. All blue examples would have colorred=0 and colorblue=1. All green examples would have colorred=0 and colorblue=0.

Now, consider the variable "raceeth" in our problem, which has levels "American Indian/Alaska Native", "Asian", "Black", "Hispanic", "More than one race", "Native Hawaiian/Other Pacific Islander", and "White". Because it is the most common in our population, we will select White as the reference level.

**2.2: Which binary variables will be included in the regression model?**\
Answer: We create a binary variable for each level except the reference level, so we would create all these variables except for raceethWhite.

