# The Analytics Edge: Assignment 2 - Linear Regression {.unnumbered}

The following link will lead you to the assignment on the edX website: [https://learning.edx.org/course/course-v1:MITx+15.071x+2T2020/block-v1:MITx+15.071x+2T2020+type\@sequential+block\@60d93a44280348d7a0a16663f92af0f7](https://learning.edx.org/course/course-v1:MITx+15.071x+2T2020/block-v1:MITx+15.071x+2T2020+type@sequential+block@60d93a44280348d7a0a16663f92af0f7)

## Climate Change

There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.

In this problem, we will attempt to study the relationship between average global temperature and several other factors.

The file [climate_change.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@climate_change.csv) contains climate data from May 1983 to December 2008.

***Start:***\
We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv into R.

```{r}
climateChange <- read.csv("/cloud/project/analyticsedge/Datasets/DatasetsUnit2/climate_change.csv")
```

Then, split the data into a *training set*, consisting of all the observations up to and including 2006, and a *testing set* consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.

```{r}
climateTrain <- subset(climateChange, Year <=2006)
climateTest <- subset(climateChange, Year > 2006)
```

Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.

```{r}
climateModel1 <- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climateTrain)
```

**1.1: What is the model R2 (the "Multiple R-squared" value)?**\
Answer: 0.7509

**1.2: Which variables are significant in the model?**\
Answer: MEI, CO2, CFC.11, CFC.12, TSI, Aerosols

```{r}
summary(climateModel1)
```

Current scientific opinion is that nitrous oxide and CFC-11 are greenhouse gases: gases that are able to trap heat from the sun and contribute to the heating of the Earth. However, the regression coefficients of both the N2O and CFC-11 variables are *negative*, indicating that increasing atmospheric concentrations of either of these two compounds is associated with lower global temperatures.

**2.1: What is the simplest correct explanation for this contradiction?**\
Answer: All of the gas concentration variables reflect human development - N2O and CFC.11 are correlated with other variables in the data set.

**2.2.1: Which of the following independent variables is N2O highly correlated with (absolute correlation greater than 0.7)?**\
Answer: CO2, CH4, CFC.12

**2.2.2: Which of the following independent variables is CFC.11 highly correlated with?**\
Answer: CH4, CFC.12

```{r}
cor(climateTrain)
```

**Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Remember to use the training set to build the model:**

```{r}
climateModel2 <- lm(Temp ~ MEI + N2O + TSI + Aerosols, data = climateTrain)
```

**3.1: What is the coefficient of N2O in this reduced model?**\
Answer: 2.532e-02 (0.02532)

**3.2: What is the model R\^2?**\
Answer: 0.7261

```{r}
summary(climateModel2)
```

We have many variables in this problem, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2. This trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model. 

**Use the step function in R to derive a new model, with the full model as the initial model:**

```{r}
climateModel <- step(climateModel1)
```

**4.1: What is the R\^2 value of the model produced by the step function?**\
Answer: 0.7508

**4.2: Which of the variable(s) were eliminated from the full model by the step function?**\
Answer: CH4

```{r}
summary(climateModel)
```

**5: Using the model produced from the step function, calculate temperature predictions for the testing data set, using the predict function. What is the testing set R\^2?**\
Answer: 0.6286051

```{r}
predictTemp <- predict(climateModel, newdata = climateTest)
SSE = sum((predictTemp - climateTest$Temp)^2)
SST = sum((mean(climateTrain$Temp) - climateTest$Temp)^2)
R2 = 1 - SSE/SST
R2
```

## Reading Test Scores

The Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in mathematics, reading, and science. This test provides a quantitative way to compare the performance of students from different parts of the world. In this homework assignment, we will predict the reading scores of students from the United States of America on the 2009 PISA exam.

The datasets [pisa2009train.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009train.csv) and [pisa2009test.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009test.csv) contain information about the demographics and schools for American students taking the exam, derived from [2009 PISA Public-Use Data Files](http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2011038) distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, by using the data you are bound by them [NCES data use agreement](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@NCES_Data_Use_Agreement.txt), which prohibits any attempt to determine the identity of any student in the datasets.

***Start:***\
Load the training and testing sets [pisa2009train.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009train.csv) and [pisa2009test.csv](https://courses.edx.org/asset-v1:MITx+15.071x+2T2017+type@asset+block@pisa2009test.csv) using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.

```{r}
pisaTrain <- read.csv("/cloud/project/analyticsedge/Datasets/DatasetsUnit2/pisa2009train.csv")
pisaTest <- read.csv("/cloud/project/analyticsedge/Datasets/DatasetsUnit2/pisa2009test.csv")
```

**1.1: How many students are there in the training set?**\
Answer: 3663
```{r}
nrow(pisaTrain)
```

**1.2.1: Using tapply() on pisaTrain, what is the average reading test score of males**\
Answer: 483.5325

**1.2.2: Using tapply() on pisaTrain, what is the average reading test score of females?**\
Answer: 512.9406
```{r}
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
```

**1.3: Which variables are missing data in at least one observation in the training set?**\
Answer: *Check output*
```{r}
names(which(colSums(is.na(pisaTrain)) > 0))
```

**Linear regression discards observations with missing data, so we will remove all such observations from the training and testing sets:**
```{r}
pisaTrain <- na.omit(pisaTrain)
pisaTest <- na.omit(pisaTest)
```


**1.4: How many observations are now in the training/testing set?**\
Answer: 2414/990 respectively
```{r}
nrow(pisaTrain)
nrow(pisaTest)
```

**2.1: Which of the variables *grade*, *male* and *raceeth* is an unordered or ordered factor with a min. of 3 values**\
Answer: 
grade - ordered (ex. 8, 9, 10, 11)
male - only has 2 values
raceeth - unordered (no way to sepcifically order it)

**How to include unordered factors in a linear regression model:**\
To include unordered factors in a linear regression model, we define one level as the "reference level" and add a binary variable for each of the remaining levels. In this way, a factor with n levels is replaced by n-1 binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.

As an example, consider the unordered factor variable "color", with levels "red", "green", and "blue". If "green" were the reference level, then we would add binary variables `colorred` and `colorblue` to a linear regression problem. All red examples would have `colorred = 1` and `colorblue = 0`. All blue examples would have `colorred = 0` and `colorblue = 1`. All green examples would have `colorred = 0` and `colorblue = 0`.

Now, consider the variable "raceeth" in our problem, which has levels "American Indian/Alaska Native", "Asian", "Black", "Hispanic", "More than one race", "Native Hawaiian/Other Pacific Islander", and "White". Because it is the most common in our population, we will select White as the reference level.

**2.2: Which binary variables will be included in the regression model?**\
Answer: We create a binary variable for each level except the reference level, so we would create all these variables except for raceethWhite.

**2.3: For a student who is Asian, which binary variables would be set to 0? What about a student who is white?**\
Answer: An Asian student will have raceethAsian set to 1 and all other raceeth binary variables set to 0. Since "White" is the reference level, a white student will have all raceeth binary variables set to 0.
\
\
Because the race variable takes on text values, it was loaded as a factor variable when we read in the dataset with read.csv() -- you can see this when you run str(pisaTrain) or str(pisaTest). However, by default R selects the first level alphabetically ("American Indian/Alaska Native") as the reference level of our factor instead of the most common level ("White").
**Set the reference level of the factor by typing the following two lines in your R console:**
```{r}
pisaTrain$raceeth = relevel(factor(pisaTrain$raceeth), "White")
pisaTest$raceeth = relevel(factor(pisaTest$raceeth), "White")
```

**Now, build a linear regression model (call it lmScore) using the training set to predict readingScore using all the remaining variables:**
```{r}
lmScore <- lm(readingScore ~ ., data = pisaTrain)
```

**3.1: What is the Multiple R-squared value of lmScore on the training set?**\
Answer: 0.3251
```{r}
summary(lmScore)
```

**3.2: What is the training-set root-mean squared error (RMSE) of lmScore?**\
Answer: 73.36555
```{r}
sqrt(mean(lmScore$residuals^2))
```

**3.3: Consider two students A and B. They have all variable values the same, except that student A is in grade 11 and student B is in grade 9. What is the predicted reading score of student A minus the predicted reading score of student B?**\
Answer: 59.08541 *(The coefficient of the variable grade is 29.542707, meaning that it affects the reading score by that value, if someone is a grade higher or lower. Since student A is 2 grades higher than student B, we have to multiply this value by 2.)*
```{r}
29.542707*2
```

**3.4: What is the meaning of the coefficient associated with variable raceethAsian?**\
Answer: Predicted difference in the reading score between an Asian student and a white student who is otherwise identical

**3.5: Based on the significance codes, which variables are candidates for removal from the model?**\
Answer: preschool, expectBachelors, motherHS, motherWork, fatherHS, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, urban
```{r}
summary(lmScore)
```


**Using the "predict" function and supplying the "newdata" argument, use the lmScore model to predict the reading scores of students in pisaTest. Call this vector of predictions "predTest":** *(Do not change the variables in the model (for example, do not remove variables that we found were not significant in the previous part of this problem))*
```{r}
predTest <- predict(lmScore, newdata = pisaTest)
```

**4.1: What is the range between the maximum and minimum predicted reading score on the test set?**\
Answer: 284.4683
```{r}
max(predTest) - min(predTest)
```

**4.2.1: What is the sum of squared errors (SSE) of lmScore on the testing set?**\
Answer: 5762082
```{r}
SSE <- sum((predTest - pisaTest$readingScore)^2)
SSE
```

**4.2.2: What is the root-mean squared error (RMSE) of lmScore on the testing set?**\
Answer: 76.29079
```{r}
sqrt(SSE/nrow(pisaTest))
```

**4.3.1: What is the predicted test score used in the baseline model?**\
Answer: 517.9629
```{r}
mean(pisaTrain$readingScore)
```

**4.3.2: What is the total sum of squares (SST) on the testing-set?**\
Answer: 7802354
```{r}
SST <- sum((mean(pisaTrain$readingScore)-pisaTest$readingScore)^2)
SST
```

**4.4: What is the test-set R-squared value of lmScore?**\
Answer: 0.2614944
```{r}
R2 <- 1 - SSE/SST
R2
```

